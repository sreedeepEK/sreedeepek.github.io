---
layout: post
title: "Run LLM's with Ollama: A Quick Guide."
description: "Getting Started with Ollama: A Guide to Downloading and Running on terminal."
tag: [tutorial]
category: articles
usemathjax: true
---

### Introduction                       
 
Ollama is an open-source project that serves as a powerful and user-friendly platform for running LLMs on your local machine. It acts as a bridge between the complexities of LLM technology and the desire for an accessible and customizable AI experience.
Key Features and Functionalities
Ollama boasts a comprehensive set of features and functionalities designed to enhance the user experience and maximize the potential of local LLMs:

### Model Library and Management 

Ollama provides access to a diverse and continuously expanding library of pre-trained LLM models, ranging from versatile general-purpose models to specialized ones tailored for specific domains or tasks. Downloading and managing these models is a seamless and streamlined process, eliminating the need to navigate complex model formats or dependencies.


In this guide, Let's walk through the process of downloading Ollama and integrating with it.

#### Step 1: Accessing Ollama on GitHub

Visit the Ollama [wesbite](https://ollama.com/download/windows).
Locate and click on "Download for windows" to download the installer.

#### Step 2: Installation Process

After downloading the installer, navigate to the folder where it's located.
Double-click on the installer to begin the installation process.
Follow the on-screen instructions and click on "Install" to proceed.

#### Step 3: Installation Completion

The installation may take a few minutes.
Note that there won't be a "Finish" button at the end.
Once the installation is complete, the installation screen will disappear automatically.

#### Step 4: Verification

Open Command Prompt (CMD) on your Windows system.
Type Ollama help in the command prompt and press Enter to check if the installation was successful.

#### Step 5: Checking Ollama Status

Open your preferred web browser.
Type localhost:11434 in the address bar and press Enter.
This will check if Ollama is up and running.

#### Step 6: Running Ollama in terminal

With Ollama successfully installed and running, you can now interact with it.
In the command prompt, type ollama pull phi or ollama pull llama2 to download the desired models and start asking questions in the terminal.



